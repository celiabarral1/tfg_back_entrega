import numpy as np


def check_installation(audio_folder, test_csv, result_csv):
    different_results = []
    
    if test_csv.size == result_csv.size:
        isCorrect = True
        
        for index, original_row in test_csv.iterrows():
            result_row = result_csv[result_csv['file_name'] == original_row['file_name']].iloc[0]
            if not original_row.equals(result_row):
                different_results.append(
                    {
                        "index": index,
                        "original": original_row,
                        "result": result_row
                    }
                )
        
        samples = len(test_csv) 
        correct_samples = samples - len(different_results)       
        accuracy = correct_samples / samples
        tolerance = 0.01
        
        report_content = f"From the {samples} samples analyzed, {correct_samples} outputs were the same ({round(accuracy * 100, 2)}%) and the following samples obtained a different output considering a tolerance of {tolerance * 100}% \n"
        
        for different_result in different_results:
            accuracy_difference , report_content = find_difference(different_result, report_content)
            
            
            if accuracy_difference > tolerance:
                isCorrect = False  
            
            
        if not isCorrect:
            report_content = f"The files are too different to be considered a successful installation. \n\n" + report_content
        else:
            report_content = f"The installation was successful. \n\n" + report_content
    else:
        report_content = "Something went wrong, as both csv files have different sizes. Try deleting the file generated by the code and execute it again."
    
    with open(audio_folder + "/../installation.txt", 'w') as file:
    # Write the string to the file
        file.write(report_content)   
        
def find_difference(different_result, report_content):
    relevant_headers = ["Emotion_1_label", "Emotion_2_label", "Emotion_3_label", "valence", "arousal", "dominance", "Emotion_1_mean", "Emotion_2_mean", "Emotion_3_mean"]
    
    irrelevant_diference = True
    
    for relevant_header in relevant_headers:
        original_expected = different_result["original"][relevant_header]
        result = different_result["result"][relevant_header]
                
        if original_expected != result:
            irrelevant_diference = False
            
            if str(type(original_expected)) == "<class 'str'>":
                accuracy_difference = 1
            else:
                accuracy_difference = (original_expected - result)/original_expected
            break
            
    if not irrelevant_diference:
        # Convert the list of strings to a NumPy array of floats
        report_content += f"\t Sample: {different_result['index']} \t Expected: {original_expected} \t Obtained: {result} \t Difference: {accuracy_difference * 100}% \n"
                         
    return accuracy_difference, report_content